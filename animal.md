Often, policy disagreements can be caused by either empirical or moral disagreements. Conflating these two causes of disagreement causes a variety of confusion and difficulty in discussion.
	
For example, I'm generally in favor of welfare payments to the poor. Morally, I think society should help poor people even if they're poor because they're lazy and stupid. Factually, I claim that giving poor people money improves their lives, because my default assumption is that people's lives are better with more money and I don't see enough evidence to convince me away from that.

There are two basic ways to disagree with me about welfare payments. You can say that morality has something to do with obligation or fairness, and for that reason redistributing money to people who didn't work for it is a bad idea. Alternatively, you can claim that welfare payments are a bad way of getting to my goal of improving the lives of poor people: maybe they get dependant on the money, or lose the self esteem that can only come from working, or something like that.

## Misunderstandings that this can create

This leads to a bunch of different misunderstandings. To start with, you can misunderstand the other person and think they're disagreeing empirically, when actually they just have different values. Slate Star Codex talks about the temptation to automatically 'steelman' arguments into utilitarianism, and says:

> I no longer try to steelman BETA-MEALR arguments as utilitarian. When I do, I just end up yelling at my interlocutor, asking how she could possibly get her calculations so wrong, only for her to reasonably protest that she wasnâ€™t make any calculations and what am I even talking about?

James A. Donald, a conservative regular on Slate Star Codex, complains about people doing this [here](http://slatestarcodex.com/2014/05/12/weak-men-are-superweapons/#comment-77187).


On the other hand, if you assume that the other person has the same beliefs as you, it's easy to think that your opponents are monsters because they'd have to be monsters to dislike the outcome you think your policy will create.

## Correlations in empirical and moral beliefs

Suspiciously, empirical beliefs and moral beliefs often seem to correlate.

For one example, take the welfare argument I started with. I bet you that people who value wellbeing instead of fairness as a terminal value are more likely to also believe that giving money to the poor doesn't hurt them, and people who care fundamentally about liberty and autonomy are more likely to claim that welfare payments lead to dependance and social problems.

As another example, the animal activism community is split over an ethical and a practical question. The moral question is whether it's fundamentally wrong to kill or "exploit" animals. Utilitarians like me say that that's fine, while some more deontological people like my friend Kelly say that all forms of using an animal for food are wrong.

The practical question is about how we should go about reducing animal suffering. Welfarists such as PETA and Peter Singer believe that we should try to make incremental steps towards reducing the amount of animal suffering in the world. For example, they support Meatless Mondays and free range meat as a way of getting to the target of basically stopping factory farming, and often do work by leafletting and online advertisements for veganism and reducing meat consumption.

In contrast, abolitionists such as Direct Action Everywhere think that the best way to reduce animal suffering is to directly confront speciesism, with demonstrations and direct attempts to challenge people's ideas that we should be able to exploit animals at all. (I'm using "abolitionist" in an unrelated sense to the way David Pearce uses it.)

These two positions correlate more than they should. In general, the welfarists are hedonistic utilitarians, and the abolitionists are deontologists.

I say that this is suspicious because there's no a priori reason that these beliefs should correlate. We get our ethics from some mysterious internal process which won't naturally lead to universal agreement, but we should hope that everyone can come to agreement on factual questions like whether the poor are harmed by giving them government money.


## Possible causes

The main explanation for this phenomenon is probably confirmation bias. Once people have a belief, they tend to accumulate more and more reasons to hold it. It's more satisfying to say "Well, I'm not trying to maximise happiness across the whole population, but even if I was, social security would be a bad idea" than "Actually, you're right, social security is a good way of helping the poor, I just don't think we need to."

Certain personality traits probably affect how we respond to both moral and empirical claims. For example, people who like math seem to be more prone to utilitarianism and arguments based on numbers. Currently, it's a lot easier to get numbers on the effectiveness of leafletting than public demonstrations. Perhaps that personality difference leads to the correlation.

And beyond just the use of numbers, the welfarists and the abolitionists talk really differently. When I first read stuff written by my abolitionist friend Kelly, her writing style reminded me of wooly-headed social justice writing, which made me automatically skeptical of her empirical claims. If one side of an argument uses language which turns off people with my personality, then that will increase the personality skew in the group.

Another reason that personality differences perpetuate themselves is that math-minded people are more motivated to make arguments which will motivate people like themselves, and likewise with rights-based people. That means that when a math person who's new to the topic sees an argument, they'll probably find the mathy side more convincing.


## Life lessons

What can we take away from this?

I wish that I had paid more attention to Kelly when I first met her. She disagrees about terminal values and doesn't talk using math jargon (for example, she doesn't use the phrase "terminal values"), but she still knows a hell of a lot about animal advocacy, and has caused me to have far more credence in the abolitionist school of animal advocacy.

So perhaps one moral should be "when you notice that someone has different terminal values to you, try to be especially attentive and receptive to their empirical claims."

Another moral is that it's very important to aim your arguments at your audience, not at yourself. For example, I haven't read a defence of the abolitionist position which is aimed at people with my terminal values. I think it might be worthwhile for me to write one.